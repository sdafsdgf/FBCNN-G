{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff7420d-3610-47f6-813e-d56b5f7f5fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 07:12:41.025619: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-18 07:12:41.203554: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-18 07:12:41.203579: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-18 07:12:41.204430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-18 07:12:41.282897: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-18 07:12:42.213515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed as np_seed, choice\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import plot_model\n",
    "from keras import utils\n",
    "from sklearn.model_selection import cross_val_score,train_test_split,KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense,LSTM, Activation, Flatten, Convolution1D, Dropout,MaxPooling1D,BatchNormalization\n",
    "from keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "#from tensorflow.keras.models import Model, Input\n",
    "from tensorflow.keras import Input, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Permute,concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv1D\n",
    "from keras.layers import Concatenate, Reshape,add\n",
    "import os\n",
    "from fb_read   import prepare_data_as, prepare_template, normalize\n",
    "import math\n",
    "from math import pi\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from functools import reduce\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc4f248-5cc1-4798-8e3e-076255185ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coff2(Layer):\n",
    "\n",
    "    def __init__(self, params, **kwargs):\n",
    "        self.tw = params['tw']\n",
    "        self.Fs = params['Fs']\n",
    "        self.cl = params['cl']\n",
    "        self.corr = None\n",
    "        super(Coff2, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # No trainable weight variable for this layer.\n",
    "        super(Coff2, self).build(input_shape)\n",
    "\n",
    "    def call(self, input, **kwargs):\n",
    "        x = input[0] # [?,tw] signal\n",
    "        t = input[1] # [?,tw,cl] reference\n",
    "        self.corr = K.sign(x) * x ** 2 + K.sign(t) * t ** 2\n",
    "        self.out = self.corr\n",
    "        #self.out = K.expand_dims(self.out, -1)\n",
    "        return self.out\n",
    "\n",
    "    # def compute_output_shape(self, input_shape):\n",
    "    #     return K.int_shape(self.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c90dab-f601-402f-b339-536cc3f1c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        print('output_dim' + str(output_dim))  # 40\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 为该层创建一个可训练的权重\n",
    "        # inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        print('input_shape'+str(input_shape))  # input_shape[(None, 40, 50), (None, 40, 50)]\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(3, input_shape[0][2], 1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        print('self.kernel.shape:' + str(self.kernel.shape))  # self.kernel.shape:(3, 50, 40)\n",
    "        super(Self_Attention, self).build(input_shape)  # 一定要在最后调用它\n",
    "\n",
    "    def call(self, x):\n",
    "        WQ = K.dot(x[0], self.kernel[0])\n",
    "        WK = K.dot(x[0], self.kernel[1])\n",
    "        WV = K.dot(x[1], self.kernel[2])\n",
    "\n",
    "        print(\"WK.shape\", WK.shape)  # WQ.shape (None, 40, 1)\n",
    "\n",
    "        print(\"K.permute_dimensions(WK, [0, 2, 1]).shape\", K.permute_dimensions(WK, [0, 2, 1]).shape)\n",
    "        # K.permute_dimensions(WK, [0, 2, 1]).shape(None, 40, 40)\n",
    "        QK = K.batch_dot(WQ, K.permute_dimensions(WK, [0, 2, 1]))  # (40, 40)\n",
    "        QK = QK / (50 ** 0.5)\n",
    "        QK = K.softmax(QK)\n",
    "        print(\"QK.shape\", QK.shape)  # QK.shape (None, 40, 40)\n",
    "        V = K.batch_dot(QK, WV)\n",
    "        print('V.shape:' + str(V.shape)) # V.shape:(None, 40, 1)\n",
    "        self.V = Reshape((40,))(V)\n",
    "\n",
    "        return self.V\n",
    "\n",
    "    # def compute_output_shape(self, input_shape):\n",
    "    #     return K.int_shape(self.V)\n",
    "## correlation analysis layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c456bd85-336c-4917-8c70-ddd1c8586351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coff(Layer):\n",
    "\n",
    "    def __init__(self, params, **kwargs):\n",
    "        self.tw = params['tw']\n",
    "        self.Fs = params['Fs']\n",
    "        self.cl = params['cl']\n",
    "        self.corr = None\n",
    "        #self.out=self.corr\n",
    "        super(Coff, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # No trainable weight variable for this layer.\n",
    "        super(Coff, self).build(input_shape)\n",
    "\n",
    "    def call(self, input, **kwargs):\n",
    "        x = input[0] # [?,tw] signal\n",
    "        t = input[1] # [?,tw,cl] reference\n",
    "        t_ = K.reshape(t,(-1,self.tw,self.cl))\n",
    "        print(t_.shape)\n",
    "\n",
    "        corr_xt = K.batch_dot(x,t_,axes=(1,1)) # [?,cl]\n",
    "        corr_xx = K.batch_dot(x,x,axes=(1,1)) # [?,1]\n",
    "        corr_tt = K.sum(t_*t_,axis=1) # [?,cl]\n",
    "        self.corr = corr_xt/K.sqrt(corr_tt)/K.sqrt(corr_xx)\n",
    "        self.out = self.corr\n",
    "        #self.out = K.expand_dims(self.out, -1)\n",
    "        return self.out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return K.int_shape(self.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e0cfac7-f5ad-4f75-b216-8bc22c2940ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(filepath='test.log', data=[], mode='w'):\n",
    "        '''\n",
    "        filepath: path to save\n",
    "        data: list of data\n",
    "        mode: a = update data to file, w = write a new file\n",
    "        '''\n",
    "        try:\n",
    "            with open(filepath, mode) as csvFile:\n",
    "                writer = csv.writer(csvFile)\n",
    "                writer.writerow(data)\n",
    "        except IOError:\n",
    "            raise Exception('I/O error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c90a9bc6-e73e-467a-961a-83dcc99b167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters\n",
    "# channels: Pz, PO5, PO3, POz, PO4, PO6, O1, Oz, O2\n",
    "permutation = [47,53,54,55,56,57,60,61,62]\n",
    "params = {'tw':250,'Fs':250,'cl':40,'ch':len(permutation)}\n",
    "# tf.random.set_seed(1122)\n",
    "# np_seed(1122)\n",
    "root_dir='./Bench/'\n",
    "all_freqs = loadmat(root_dir+'Freq_Phase.mat')['freqs'][0] # true fre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d3640c3-c5cb-4b51-9481-cff2e18af551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['tw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cbe92-4ef7-41cf-adc3-39efa8496064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject id is: 1\n",
      "epoch第1次\n",
      "111\n",
      "(1040, 10, 250, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 07:13:48.020717: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.03365, saving model to ./model/S1_250.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy improved from 0.03365 to 0.20288, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.20288 to 0.54327, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.54327 to 0.58077, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.58077 to 0.68462, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.68462 to 0.73462, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.73462 to 0.77019, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.77019 to 0.78846, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.78846 to 0.82115, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.82115 to 0.82692, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.82692 to 0.84615, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.84615 to 0.85192, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.85192\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.85192\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.85192 to 0.85769, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.85769\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.85769\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.85769\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.85769 to 0.86442, saving model to ./model/S1_250.h5\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.86442\n",
      "load successed\n",
      "33/33 [==============================] - 1s 19ms/step - loss: 0.6954 - accuracy: 0.8644\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "33/33 [==============================] - 1s 19ms/step\n",
      "Test loss: 0.6953792572021484\n",
      "Test accuracy: 0.8644230961799622\n",
      "epoch第2次\n",
      "S1|x1 (5200, 250, 9)\n",
      "S1|x2 (5200, 250, 9)\n",
      "S1|x3 (5200, 250, 9)\n",
      "S1|x1 (1040, 250, 9)\n",
      "S1|x2 (1040, 250, 9)\n",
      "S1|x3 (1040, 250, 9)\n",
      "y_test= [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "S1|template (1040, 40, 250, 9)\n",
      "S1|template2 (1040, 40, 250, 9)\n",
      "S1|template3 (1040, 40, 250, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "(1040, 10, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n",
      "(None, 250, 40)\n"
     ]
    }
   ],
   "source": [
    "for subj in range(1,36):\n",
    "    print('subject id is: '+str(subj))\n",
    "    #train_run = [1,2,3,4,0]\n",
    "    #test_run = [5]\n",
    "    run=[0,1,2,3,4,5]\n",
    "    for i in range(len(run)):\n",
    "        test_run=[run[i]]\n",
    "        train_run=run[:i]+run[i+1:]\n",
    "        print(f'epoch第{i+1}次')\n",
    "        # for k, (train_run, test_run) in enumerate(kfold):\n",
    "        #print(test_run)\n",
    "        file_list = [root_dir+\"/x_train1_file\" + str(subj) + \"_\" + str(params['tw']) + \"_\" + str(test_run) + \".npy\",\n",
    "                     root_dir+\"/x_train2_file\" + str(subj) + \"_\" + str(params['tw']) + \"_\" + str(test_run) + \".npy\",\n",
    "                     root_dir+\"/x_train3_file\" + str(subj) + \"_\" + str(params['tw']) + \"_\" + str(test_run) + \".npy\",\n",
    "                     root_dir+\"/y_train_file\" + str(subj) + \"_\" + str(params['tw']) + \"_\" + str(test_run) + \".npy\",\n",
    "                     root_dir+\"/x_test1_file\" + str(subj) + \"_\" + str(params['tw']) + \"_\" + str(test_run) + \".npy\",\n",
    "                     root_dir+\"/x_test2_file\" + str(subj) + \"_\" + str(params['tw']) + \"_\" + str(test_run) + \".npy\",\n",
    "                     root_dir+\"/x_test3_file\" + str(subj) + \"_\" + str(params['tw']) + \"_\" + str(test_run) + \".npy\",\n",
    "                     root_dir+\"/y_test_file\" + str(subj) + \"_\" + str(params['tw']) + \"_\" + str(test_run) + \".npy\",\n",
    "                     root_dir+\"/template1_file\" + str(subj) + \"_\" + str(params['tw']) + \"_\" + str(test_run) + \".npy\",\n",
    "                     root_dir+\"/template2_file\" + str(subj) + \"_\" + str(params['tw']) + \"_\" + str(test_run) + \".npy\",\n",
    "                     root_dir+\"/template3_file\" + str(subj) + \"_\" + str(params['tw']) + \"_\" + str(test_run) + \".npy\",\n",
    "                     ]\n",
    "        x_train1_file, x_train2_file,x_train3_file,yy_train_file, x_test1_file,x_test2_file,x_test3_file, yy_test_file, template1_file,template2_file,template3_file = file_list\n",
    "        if not reduce(lambda x, y: os.path.exists(x) and os.path.exists(y), file_list):\n",
    "            x_train1,x_train2,x_train3 ,y_train,freq = prepare_data_as(subj,train_run,params['tw']) ## [?,tw,ch]\n",
    "            x_test1,x_test2,x_test3,y_test,__ = prepare_data_as(subj,test_run,params['tw']) # [?,tw,ch]\n",
    "    \n",
    "            x_train1 = x_train1.reshape((x_train1.shape[0],params['tw'],params['ch'],1))\n",
    "            x_test1 = x_test1.reshape((x_test1.shape[0],params['tw'],params['ch'],1))\n",
    "            x_train2 = x_train2.reshape((x_train2.shape[0], params['tw'], params['ch'], 1))\n",
    "            x_test2 = x_test2.reshape((x_test2.shape[0], params['tw'], params['ch'], 1))\n",
    "            x_train3 = x_train3.reshape((x_train3.shape[0], params['tw'], params['ch'], 1))\n",
    "            x_test3 = x_test3.reshape((x_test3.shape[0], params['tw'], params['ch'], 1))\n",
    "            y_train = keras.utils.to_categorical(y_train, params['cl'])\n",
    "            y_test = keras.utils.to_categorical(y_test, params['cl'])\n",
    "            print(\"y_test=\",y_test)\n",
    "            ## build reference signal\n",
    "            template, template2, template3 = prepare_template(subj, train_run, params['tw'])  # [cl*sample,cl,tw,ch]\n",
    "            template = np.transpose(template, axes=(0, 2, 1, 3))  # [cl*sample,tw,cl,ch]\n",
    "            template2 = np.transpose(template2, axes=(0, 2, 1, 3))  # [cl*sample,tw,cl,ch]\n",
    "            template3 = np.transpose(template3, axes=(0, 2, 1, 3))  # [cl*sample,tw,cl,ch]\n",
    "            data_list = [x_train1,x_train2,x_train3, y_train, x_test1,x_test2,x_test3, y_test, template,template2,template3]\n",
    "            for i, data in enumerate(data_list):\n",
    "                np.save(file_list[i], data)\n",
    "        else:\n",
    "                x_train1,x_train2,x_train3, y_train, x_test1,x_test2,x_test3, y_test, template,template2,template3 = [np.load(file) for file in file_list]\n",
    "    \n",
    "    \n",
    "        freq = np.array(all_freqs)\n",
    "        # Y = K.zeros(10, params['tw'], params['ch']);\n",
    "        Y = np.zeros([10, params['tw'], 40], np.float64)\n",
    "        pha_val = np.array([0, 0.5, 1, 1.5, 0, 0.5, 1, 1.5, 0, 0.5, 1, 1.5, 0, 0.5, 1, 1.5, 0, 0.5, 1, 1.5\n",
    "                               , 0, 0.5, 1, 1.5, 0, 0.5, 1, 1.5, 0, 0.5, 1, 1.5, 0, 0.5, 1, 1.5, 0, 0.5, 1, 1.5]) * math.pi\n",
    "        sig_len = params['tw'] * params['Fs']\n",
    "        t = [i / 1000 for i in range(4, 1001, 4)]\n",
    "        t = np.array(t)\n",
    "        for targ_i in range(0, 40):\n",
    "            # for Nhar in range(1, 2):\n",
    "            # print( 2 * math.pi * freq[targ_i] * 1 * pha_val[targ_i] )\n",
    "            Y0 = np.array([np.sin(2 * math.pi * 1 * freq[targ_i] * t + 1 * pha_val[targ_i]),\n",
    "                               np.cos(2 * math.pi * 1 * freq[targ_i] * t + 1 * pha_val[targ_i]),\n",
    "                               np.sin(2 * math.pi * 2 * freq[targ_i] * t + 2 * pha_val[targ_i]),\n",
    "                               np.cos(2 * math.pi * 2 * freq[targ_i] * t + 2 * pha_val[targ_i]),\n",
    "                               np.sin(2 * math.pi * 3 * freq[targ_i] * t + 3 * pha_val[targ_i]),\n",
    "                               np.cos(2 * math.pi * 3 * freq[targ_i] * t + 3 * pha_val[targ_i]),\n",
    "                               np.sin(2 * math.pi * 4 * freq[targ_i] * t + 4 * pha_val[targ_i]),\n",
    "                               np.cos(2 * math.pi * 4 * freq[targ_i] * t + 4 * pha_val[targ_i]),\n",
    "                               np.sin(2 * math.pi * 5 * freq[targ_i] * t + 5 * pha_val[targ_i]),\n",
    "                               np.cos(2 * math.pi * 5 * freq[targ_i] * t + 5 * pha_val[targ_i]),\n",
    "                           ])\n",
    "            # print(Y0.shape)\n",
    "            Y[:, :, targ_i] = Y0;\n",
    "            Y0 = [];\n",
    "        #print(Y.shape)  # 10 50 40\n",
    "        T1 = np.zeros([1040, 10, params['tw'], 40], np.float64)\n",
    "        for i in range(0, 1040):\n",
    "            T1[i, :, :, :] = Y;\n",
    "        print(111)\n",
    "        print(T1.shape)\n",
    "        # T1 = T1.reshape((T1.shape[0], T1.shape[2], T1.shape[3], 10))\n",
    "        T1 = T1.transpose(0, 2, 3, 1)\n",
    "        #x_train = np.concatenate((x_train1, x_train2, x_train3), axis=-1)\n",
    "    \n",
    "        #    y_train = np.reshape(y_train, 3)\n",
    "    \n",
    "        ## build model\n",
    "        # signal-CNN\n",
    "        signal1 = Input(shape=(params['tw'],params['ch'],1))\n",
    "        signal2 = Input(shape=(params['tw'], params['ch'], 1))\n",
    "        signal3 = Input(shape=(params['tw'], params['ch'], 1))\n",
    "        temp1 = Input(shape=(params['tw'], params['cl'], params['ch']))\n",
    "        temp2 = Input(shape=(params['tw'], params['cl'], params['ch']))\n",
    "        temp3 = Input(shape=(params['tw'], params['cl'], params['ch']))\n",
    "        Y = Input(shape=(params['tw'], params['cl'], 10))\n",
    "        conv11 = Conv2D(16, (9, 9), padding='same')(signal1)\n",
    "        conv12 = Conv2D(1, (1, 9), padding='same')(conv11)\n",
    "        conv13 = Conv2D(1, (1, 9), padding='valid')(conv12)\n",
    "        drop1 = Dropout(0.5)(conv13)\n",
    "        flat1 = Flatten()(drop1)\n",
    "        flat11 = Flatten()(conv13)\n",
    "        # reference-CNN\n",
    "        conv21 = Conv2D(40, (9, 1), padding='same')(temp1)\n",
    "        conv22 = Conv2D(1, (9, 1), padding='same')(conv21)\n",
    "        drop2 = Dropout(0)(conv22)\n",
    "        ##\n",
    "        #conv21 = Conv2D(40, (1, 1), padding='same')(Y)\n",
    "        # print(\"conv21.shape:\" + str(conv21.shape))  # conv21.shape:(?, 50, 40, 40)\n",
    "        conv31 = Conv2D(1, (1, 1), padding='same')(Y)\n",
    "        #print(conv31)\n",
    "        #print(flat11)\n",
    "        m = Coff(params)([flat11, conv31])\n",
    "        corr1 = Coff(params)([flat1, drop2])\n",
    "        corr1 = Coff2(params)([m, corr1])\n",
    "        ####################################\n",
    "        conv11 = Conv2D(16, (9, 9), padding='same' )(signal2)\n",
    "        conv12 = Conv2D(1, (1, 9), padding='same' )(conv11)\n",
    "        conv13 = Conv2D(1, (1, 9), padding='valid' )(conv12)\n",
    "        drop1 = Dropout(0.5)(conv13)\n",
    "        flat1 = Flatten()(drop1)\n",
    "        flat11 = Flatten()(conv13)\n",
    "        # reference-CNN\n",
    "        conv21 = Conv2D(40, (9, 1), padding='same' )(temp2)\n",
    "        conv22 = Conv2D(1, (9, 1), padding='same' )(conv21)\n",
    "        drop2 = Dropout(0 )(conv22)\n",
    "    \n",
    "        #conv21 = Conv2D(40, (1, 1), padding='same')(Y)\n",
    "        # print(\"conv21.shape:\" + str(conv21.shape))  # conv21.shape:(?, 50, 40, 40)\n",
    "        conv31 = Conv2D(1, (1, 1), padding='same')(Y)\n",
    "        m2 = Coff(params)([flat11, conv31])\n",
    "        corr2 = Coff(params)([flat1, drop2])\n",
    "        corr2 = Coff2(params)([m2, corr2])\n",
    "        ##############################################################\n",
    "        conv11 = Conv2D(16, (9, 9), padding='same')(signal3)\n",
    "        conv12 = Conv2D(1, (1, 9), padding='same')(conv11)\n",
    "        conv13 = Conv2D(1, (1, 9), padding='valid')(conv12)\n",
    "        drop1 = Dropout(0.5)(conv13)\n",
    "        flat1 = Flatten()(drop1)\n",
    "        flat11 = Flatten()(conv13)\n",
    "        # reference-CNN\n",
    "        conv21 = Conv2D(40, (9, 1), padding='same')(temp3)\n",
    "        conv22 = Conv2D(1, (9, 1), padding='same')(conv21)\n",
    "        drop2 = Dropout(0 )(conv22)\n",
    "        #conv21 = Conv2D(40, (1, 1), padding='same')(Y)\n",
    "        # print(\"conv21.shape:\" + str(conv21.shape))  # conv21.shape:(?, 50, 40, 40)\n",
    "        conv31 = Conv2D(1, (1, 1), padding='same')(Y)\n",
    "        m3 = Coff(params)([flat11, conv31])\n",
    "        corr3 = Coff(params)([flat1, drop2])\n",
    "        corr3 = Coff2(params)([m3, corr3])\n",
    "        out=keras.layers.concatenate([corr1,corr2,corr3 ])\n",
    "    \n",
    "        out = Dense(params['cl'], activation='softmax')(out)\n",
    "        #print(\"out.shape:\" + str(out.shape))  # out.shape:(?, 40)\n",
    "        model = Model(inputs=[signal1,signal2,signal3, temp1,temp2,temp3,Y],outputs=out)\n",
    "        # 有一次提升, 则覆盖一次.\n",
    "        model_path = './model/S'+str(subj)+'_'+str(params['tw'])+'.h5'\n",
    "    \n",
    "        #model_path = './fb_sign250S31_250.h5'\n",
    "        model_checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "        opt = keras.optimizers.Adam(lr=0.0008, beta_1=0.9, beta_2=0.999, clipvalue=5.)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=opt,\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "        #print(model.summary())\n",
    "    \n",
    "        # train & test\n",
    "        #x_train = x_train.astype('float32')\n",
    "        #x_test = x_test.astype('float32')\n",
    "        #print(y_train.shape)\n",
    "        #print(np.tile(template2, (len(train_run),1,1,1)).shape)\n",
    "        h=model.fit([x_train1 ,x_train2,x_train3 , np.tile(template, (len(train_run),1,1,1)), np.tile(template2, (len(train_run),1,1,1)),np.tile(template3, (len(train_run),1,1,1)),np.tile(T1, (len(train_run),1,1,1)) ], y_train, batch_size=32, epochs=20,\n",
    "            validation_data=([x_test1 ,x_test2,x_test3 , template ,template2,template3,T1], y_test), shuffle=True,callbacks=[model_checkpoint],verbose=0)\n",
    "        # h=model.fit([x_train1 ,x_train2,x_train3 , np.tile(template, (len(train_run),1,1,1)), np.tile(template2, (len(train_run),1,1,1)),np.tile(template3, (len(train_run),1,1,1)),np.tile(T1, (len(train_run),1,1,1)) ], y_train, batch_size=32, epochs=0,\n",
    "        #     validation_data=([x_test1 ,x_test2,x_test3 , template ,template2,template3,T1], y_test), shuffle=True,verbose=0)\n",
    "        #print(111)\n",
    "        #print(111)\n",
    "        model.load_weights(model_path)\n",
    "    \n",
    "        print(\"load successed\")\n",
    "        # Score trained model.\n",
    "        scores = model.evaluate([x_test1 ,x_test2,x_test3 , template ,template2,template3,T1], y_test, verbose=1)\n",
    "        y_pred = model.predict([x_test1 ,x_test2,x_test3 , template ,template2,template3,T1], batch_size=32)\n",
    "        evaluation = {\n",
    "                      'subject': subj,\n",
    "                      'loss': scores[0],\n",
    "                      # 'classifier_loss': classifier_loss,\n",
    "                      'accuracy': scores[1],\n",
    "                       }\n",
    "    \n",
    "        print('Test loss:', scores[0])\n",
    "        print('Test accuracy:', scores[1])\n",
    "        csv_file = './fb_sign50.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85980819-3b5f-44d7-afe1-aa3b576d2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import cheb1ord, filtfilt, cheby1\n",
    "file = loadmat('Bench/'+'Sub_1.mat')['data']\n",
    "[N_channel,N_point, N_target, N_block] = np.shape(file)\n",
    "fs=250;\n",
    "file1=file\n",
    "Wp = [6/fs*2, 90/fs*2]\n",
    "Ws = [4/fs*2, 95/fs*2]\n",
    "N, Wn=cheb1ord(Wp, Ws, 6, 18)\n",
    "b, a = cheby1(N, 0.5, Wn,'bandpass')\n",
    "for bloc_i in range(0,N_block):\n",
    "        for targ_i in range(0,N_target):\n",
    "            for chan_i in range(0,N_channel):\n",
    "                file1[chan_i,:,targ_i,bloc_i] = filtfilt(b,a,file[chan_i,:,targ_i,bloc_i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14b3ed49-34d9-4ce4-8d63-ec7071b4fa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 250, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor shape=(None, 40), dtype=float32, sparse=False, name=keras_tensor_136>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Coff(params)([flat11, conv31])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16b8885a-dfcb-46f0-b96a-d87b2d15f68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor shape=(None, 250), dtype=float32, sparse=False, name=keras_tensor_131>,\n",
       " <KerasTensor shape=(None, 250, 40, 1), dtype=float32, sparse=False, name=keras_tensor_135>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[flat11, conv31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5fd422b0-2270-4054-8d70-ff33b4aaac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.3\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "89f97c58-b989-409c-bd8c-8e1f365efab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56788e76-f337-404d-8cdb-e32774912ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 50, 9, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "968e1a0f-c884-4235-83fb-4bbabbb333bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 40)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0706894-0de3-4fd0-ba8c-2de9d0229f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3010424a-84d1-4e8b-9ed5-b32c58bd43f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_splits=4 cannot be greater than the number of members in each class.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m kfold \u001b[38;5;241m=\u001b[39m strtfdKFold\u001b[38;5;241m.\u001b[39msplit(run,run)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#循环迭代，（K-1）份用于训练，1份用于验证，把每次模型的性能记录下来。\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_run\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkfold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:377\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m         (\n\u001b[1;32m    372\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    374\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[1;32m    375\u001b[0m     )\n\u001b[0;32m--> 377\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:108\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    106\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m    107\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(_num_samples(X))\n\u001b[0;32m--> 108\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_test_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_not\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:770\u001b[0m, in \u001b[0;36mStratifiedKFold._iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_test_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 770\u001b[0m     test_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_test_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits):\n\u001b[1;32m    772\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m test_folds \u001b[38;5;241m==\u001b[39m i\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:732\u001b[0m, in \u001b[0;36mStratifiedKFold._make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    730\u001b[0m min_groups \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(y_counts)\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m y_counts):\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_splits=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m cannot be greater than the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of members in each class.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits)\n\u001b[1;32m    735\u001b[0m     )\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m min_groups:\n\u001b[1;32m    737\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    738\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m members, which is less than n_splits=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    740\u001b[0m         \u001b[38;5;241m%\u001b[39m (min_groups, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits),\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    742\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: n_splits=4 cannot be greater than the number of members in each class."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d3f96-559a-43ab-91fd-9f611686c05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
